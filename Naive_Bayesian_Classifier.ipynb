{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seperating by outcme: Done\n",
      "starting clustering by month:\n",
      "done\n",
      "started droping unnecessary rows:\n",
      "done\n",
      "started droping unnecessary columns:\n",
      "For Decision Tree ,\n",
      "\n",
      "\t After cross validation the  different scores are as follows:\n",
      "recall: [0.63577586 0.63577586 0.63577586 0.59482759 0.60344828 0.65086207\n",
      " 0.62284483 0.62068966 0.63362069 0.61637931] \n",
      " precision: [0.5353902  0.52304965 0.51304348 0.49819495 0.47945205 0.52521739\n",
      " 0.4923339  0.51428571 0.51488616 0.49225473] \n",
      " accuracy: [0.82247285 0.81704261 0.81244779 0.80526536 0.79607188 0.81821981\n",
      " 0.80234016 0.8127873  0.81320518 0.80234016] \n",
      " fscore: [0.58128079 0.57392996 0.56785371 0.54223969 0.53435115 0.5813282\n",
      " 0.54995243 0.5625     0.56811594 0.54736842] \n",
      "\n",
      "recall mean   : 0.625 with starderd deviation: 0.016012220362936813\n",
      "precision mean: 0.5088108228555484 with starderd deviation: 0.016012220362936813\n",
      "accuracy mean : 0.8102193078461581 with starderd deviation: 0.016012220362936813\n",
      "fscore   mean : 0.5608920274984014 with starderd deviation: 0.016012220362936813\n",
      "Time taken: 189.57768845558167\n"
     ]
    }
   ],
   "source": [
    "##importing necessary libraries\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import sklearn.tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## RETURNS percentage% OF DATAFrame AFTER SAMPLING\n",
    "\n",
    "def sampling(dataframe,percentage):\n",
    "    \n",
    "    return dataframe.sample(frac=percentage).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "## PRE-PROCESSING:\n",
    "## CONVERTS WORD CLASSES TO NUMERIC CLASSES:\n",
    "def preprocess_word_to_num(dataframe):\n",
    "    df_copy=dataframe.copy(deep=True)\n",
    "       \n",
    "    for column in df_copy.columns:\n",
    "        if column==\"y\":\n",
    "            df_copy[column]=df_copy[column].replace({\"yes\":1,\"no\":0}) \n",
    "        if column==\"job\":\n",
    "            df_copy[\"job\"].replace({\"management\":1,\n",
    "                                    \"blue-collar\":2,\n",
    "                                    \"admin\":3,\n",
    "                                    \"entrepreneur\":4,\n",
    "                                    \"services\":5,\n",
    "                                    \"self-employed\":6,\n",
    "                                    \"retired\":7,\n",
    "                                    \"technician\":8,\n",
    "                                    \"housemaid\":9,\n",
    "                                    \"student\":10,\n",
    "                                    \"unemployed\":11,\n",
    "                                    \"unknown\":12})\n",
    "        value=df_copy[column][df_copy.first_valid_index()]\n",
    "   \n",
    "        if not (isinstance(value, np.int64) or isinstance(value, np.float64)):\n",
    "            l1=list(set(df_copy[column]))\n",
    "            l2=list(range(0,len(list(set(df_copy[column])))))\n",
    "            df_copy[column]=df_copy[column].replace(l1,l2)\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "##NORMALIZES THE FEATURE DATA POINTS BY REPLACING X WITH (X-X_MEAN)/X_STD\n",
    "\n",
    "def normalize(dataframe):\n",
    "        df_copy=dataframe.copy(deep=True)\n",
    "        for column in df_copy.columns:\n",
    "            std_dev=np.std(df_copy[column])\n",
    "            mean=np.mean(df_copy[column])\n",
    "            l1=list(set(df_copy[column]))\n",
    "            l2=list((item-mean)/std_dev for item in l1)\n",
    "            df_copy[column]=df_copy[column].replace(l1,l2)\n",
    "        return df_copy\n",
    "\n",
    "    \n",
    "##DROPS UNNECESSARY FEARURE FFROM TRAINING DATA:\n",
    "\n",
    "def drop_column(dataframe,col):\n",
    "    return dataframe.drop(col,axis=1)\n",
    "\n",
    "\n",
    "#counting time from now\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "#redaing data from csv\n",
    "\n",
   "url=\"https://github.com/shadow23-cmi/DT-SVM-NB/raw/master/bank-additional-full.csv\"\n",
    "df1=pd.read_csv(url,sep=\";\")\n",    "\n",
    "#storing all the distinct months and distinct consumer price index\n",
    "# as, dividing the data points on maothly basis needs these 2 parameters\n",
    "#  as, there are more than 1 year and no 2 same month of diff yaer have same same consumer price index\n",
    "\n",
    "month_unique=list(set(df1[\"month\"]))\n",
    "cons_price_idx_unique=list(set(df1[\"cons.price.idx\"]))\n",
    "\n",
    "\n",
    "\n",
    "#seperating on basis of outcome\n",
    "#so that the majority class can be undersampled for better fitting\n",
    "\n",
    "df_no=df1[df1[\"y\"]==\"no\"]\n",
    "df_yes=df1[df1[\"y\"]==\"yes\"]\n",
    "\n",
    "print(\"Seperating by outcme: Done\")\n",
    "\n",
    "\n",
    "#reseting df_no indices\n",
    "\n",
    "df_no=df_no.reset_index(drop=True)\n",
    "\n",
    "#taking only the following features while calculating data point distances\n",
    "# as, taking monthly data for clustering the rest of the features are of no need as they are monthly data\n",
    "#  so, no difference is made\n",
    "\n",
    "df_no_undersampling_clustered=df_no[[\"age\", 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
    "                                    'contact', 'day_of_week', \"month\", 'campaign', 'pdays',\"cons.price.idx\"]]\n",
    "\n",
    "print(\"starting clustering by month:\")\n",
    "\n",
    "\n",
    "#this object will give distance matrix based on \"taxi cab \" matrix:\n",
    "\n",
    "dist = DistanceMetric.get_metric('manhattan')\n",
    "\n",
    "# contains the ps than can be deleted for under-sampling\n",
    "# and their centers\n",
    "\n",
    "pts_to_delete=[]\n",
    "centers=[]\n",
    "\n",
    "# Here we are calculating the distance matrix of monthly data pts\n",
    "#  since, the distance matrix is symmetric we are considering only the upper triangular part\n",
    "#   there, the pts that  have >= 4 pts in therir 2 radius neighbourhood are only selected \n",
    "#    and, the rst of the pts in its neighbourhood are discarded\n",
    "for month in month_unique:\n",
    "    for index in cons_price_idx_unique:\n",
    "        \n",
    "        \n",
    "        df_month_wise=df_no_undersampling_clustered.loc[(df_no_undersampling_clustered[\"month\"] == month )\n",
    "                                                        &(df_no_undersampling_clustered[\"cons.price.idx\"]==index)]\n",
    "        start_index=df_month_wise.first_valid_index()\n",
    "               \n",
    "        if not df_month_wise.empty:\n",
    "            \n",
    "            df_month_wise=preprocess_word_to_num(df_month_wise)\n",
    "            distance_matrix=dist.pairwise(df_month_wise)\n",
    "            \n",
    "            for i in range(len(distance_matrix)):\n",
    "                \n",
    "                temp=[]\n",
    "                \n",
    "                for j in range(i+1,len(distance_matrix)):\n",
    "                    \n",
    "                    if distance_matrix[i][j]==0:\n",
    "                        pts_to_delete.append(j+start_index)\n",
    "                    \n",
    "                    if distance_matrix[i][j]<=2:\n",
    "                        temp.append(j+start_index)        \n",
    "                \n",
    "                if len(temp)>=4:\n",
    "                    pts_to_delete += temp\n",
    "                    centers.append(i+start_index)\n",
    "\n",
    "\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "#storing unnecessary data pts\n",
    "\n",
    "drop=list((set(pts_to_delete)))\n",
    "\n",
    "\n",
    "print(\"started droping unnecessary rows:\")\n",
    "\n",
    "#Dropping the unnecesary data pts after clustering\n",
    "\n",
    "df_no.drop(df_no.index[drop],inplace=True)\n",
    "print(\"done\")\n",
    "df_no=df_no.reset_index(drop=True)\n",
    "\n",
    "#taking 80% the no clss data\n",
    "\n",
    "df_no=sampling(df_no,0.8)\n",
    "\n",
    "#creating mixed data types by combining all yes class and no clss data\n",
    "#randomsampling them\n",
    "\n",
    "df=df_no.append(df_yes)\n",
    "df=df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#Pre processing data by sampling and converting str data types to numerical types:\n",
    "\n",
    "df=sampling(df,1)\n",
    "df=preprocess_word_to_num(df)\n",
    "\n",
    "print(\"started droping unnecessary columns:\")\n",
    "\n",
    "#Dropping unnecessary features\n",
    "\n",
    "df2=drop_column(df,\"y\")\n",
    "\n",
    "#Normalizing the features to mean 0 standerd deviation 1\n",
    "\n",
    "df2=normalize(df2)\n",
    "\n",
    "\n",
    "# Final Data for fitting:\n",
    "\n",
    "data=df2.to_numpy()\n",
    "target=df[\"y\"].to_numpy()\n",
    "\n",
    "#Fitting naive bayes classifire\n",
    "naive_bayes=GaussianNB()\n",
    "\n",
    "data_train,data_test,target_train,target_test=train_test_split(data,target,test_size=.1,shuffle=True)\n",
    "\n",
    "\n",
    "crs=cross_validate(naive_bayes, \n",
    "                     data, \n",
    "                     target,\n",
    "                     cv=10,\n",
    "                     scoring=[\"recall\",\"precision\",\"accuracy\",\"f1\"],\n",
    "                     n_jobs=2)\n",
    "\n",
    "#printing output:\n",
    "\n",
    "print(\"For Naive bayesian ,\\n\")\n",
    "print(\"\\t After cross validation the  different scores are as follows:\")\n",
    "print(\"recall:\",crs[\"test_recall\"],\"\\n\",\n",
    "      \"precision:\",crs[\"test_precision\"],\"\\n\",\n",
    "      \"accuracy:\",crs[\"test_accuracy\"],\"\\n\",\n",
    "      \"fscore:\",crs[\"test_f1\"],\"\\n\")\n",
    "print(\"recall mean   :\", crs[\"test_recall\"].mean(),\"with starderd deviation:\",crs[\"test_recall\"].std())\n",
    "print(\"precision mean:\",crs[\"test_precision\"].mean(),\"with starderd deviation:\",crs[\"test_recall\"].std())\n",
    "print(\"accuracy mean :\",crs[\"test_accuracy\"].mean(),\"with starderd deviation:\",crs[\"test_recall\"].std())\n",
    "print(\"fscore   mean :\",crs[\"test_f1\"].mean(),\"with starderd deviation:\",crs[\"test_recall\"].std())\n",
    "\n",
    "end=time.time()\n",
    "total_time=end-start\n",
    "# out put file:\n",
    "\n",
    "with open(\"output_file.txt\",\"a+\")as output_file:\n",
    "    output_file.write(\"\\nFor Naive Bayesian ,\\n\")\n",
    "    output_file.write(\"\\t After cross validation the  different scores are as follows:\")\n",
    "\n",
    "    output_file.write(\"\\n\\n\\t accuracy: \\t\")\n",
    "    output_file.write(str(crs[\"test_accuracy\"]))\n",
    "\n",
    "    output_file.write(\"\\n\\n\\t recall: \\t\")\n",
    "\n",
    "    output_file.write(str(crs[\"test_recall\"])) \n",
    "    output_file.write(\"\\n\\n\\t precision: \\t\")\n",
    "\n",
    "\n",
    "    output_file.write(str(crs[\"test_precision\"])) \n",
    "    output_file.write(\"\\n\\n\\t fscore: \\t\")\n",
    "\n",
    "    output_file.write(str(str(crs[\"test_f1\"])))                  \n",
    "\n",
    "    x=\"\\n\\t recall mean   :\\t\"+str(round(crs[\"test_recall\"].mean(),2))+\"\\t with starderd deviation:\\t\"+str(round(crs[\"test_recall\"].std(),3))\n",
    "    output_file.write(x)\n",
    "    y=\"\\n\\t precision mean:\\t\"+str(round(crs[\"test_precision\"].mean(),1))+\"\\t with starderd deviation:\\t\"+str(round(crs[\"test_recall\"].std(),3))\n",
    "    output_file.write(y)\n",
    "    z=\"\\n\\t accuracy mean :\\t\"+str(round(crs[\"test_accuracy\"].mean(),2))+\"\\t with starderd deviation:\\t\"+str(round(crs[\"test_recall\"].std(),3))\n",
    "    output_file.write(z)\n",
    "    f1=\"\\n\\t fscore mean   :\\t\"+str(round(crs[\"test_f1\"].mean(),2))+\"\\t with starderd deviation:\\t\"+str(round(crs[\"test_f1\"].std(),3))\n",
    "    output_file.write(f1)\n",
    "    output_file.write(\"\\n\")\n",
    "    output_file.write(\"Time taken: \"+str(total_time)+\"secs\")\n",
    "    output_file.write(\"\\n ===================================================================\")\n",
    "\n",
    "print(\"Time taken:\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
